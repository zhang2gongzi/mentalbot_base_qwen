import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import re

# ========== 1. æ¨¡å‹åŠ è½½ ==========
@st.cache_resource
def load_model():
    with st.spinner("ğŸ§  åŠ è½½Qwen3-8Bæ¨¡å‹..."):
        model_path = "/home2/zzl/model/Qwen3-8B"
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    return model, tokenizer

model, tokenizer = load_model()

# ========== 2. å¼ºçº¦æŸæç¤ºè¯ ==========
SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€åä¸“ä¸šå¿ƒç†å’¨è¯¢å¸ˆã€‚è¯·ä¸¥æ ¼éµå®ˆï¼š\n"
    "â€¢ åªè¾“å‡ºæœ€ç»ˆå›å¤ï¼Œç¦æ­¢ä»»ä½•æ€è€ƒè¿‡ç¨‹ã€å†…éƒ¨æ¨ç†\n"
    "â€¢ ç¦æ­¢è¾“å‡º <think> æ ‡ç­¾ã€æ‹¬å·å†…å®¹æˆ–æ€è€ƒå¼€å¤´\n"
    "â€¢ ä¿æŒæ¸©æš–ä¸“ä¸šï¼Œæ¯æ¬¡å›å¤ä¸è¶…è¿‡5å¥è¯"
    "â€¢ ä»¥å€¾å¬è€…å’Œæ”¯æŒè€…èº«ä»½æä¾›å¸®åŠ©\n"
    "â€¢ ä¸åšè¯„åˆ¤ï¼Œåªæä¾›æƒ…æ„Ÿæ”¯æŒ\n"
    "â€¢ ä¼˜å…ˆå…³æ³¨ç”¨æˆ·å½“ä¸‹çš„æƒ…ç»ªçŠ¶æ€\n"
    "â€¢ å¿…è¦æ—¶æä¾›ä¸“ä¸šæ±‚åŠ©æ¸ é“\n\n"
)

# ========== 3. åˆå§‹åŒ–å¯¹è¯å†å² ==========
if "messages" not in st.session_state:
    st.session_state.messages = []

# ========== 4. ä¾§è¾¹æ  ==========
with st.sidebar:
    if st.button("ğŸ”„ æ¸…é™¤å¯¹è¯"):
        st.session_state.messages = []
        st.rerun()
    st.markdown("### ğŸ†˜ ç´§æ€¥æ±‚åŠ©")
    st.markdown("`400-161-9995` å…¨å›½å¿ƒç†æ´åŠ©çƒ­çº¿")
    st.markdown("`12356` å››å·çœå¿ƒç†æ´åŠ©çƒ­çº¿")

# ========== 5. ä¸»ç•Œé¢ ==========
st.title("ğŸ’¬ å¿ƒç†å’¨è¯¢åŠ©æ‰‹")
st.caption("ğŸ”’ å®‰å…¨ç§å¯† Â· è¿™é‡Œæ˜¯ä¸€ä¸ªå®‰å…¨ã€ç§å¯†çš„ç©ºé—´ï¼Œä½ å¯ä»¥è‡ªç”±åˆ†äº«æƒ³æ³•å’Œæ„Ÿå—ï¼Œæˆ‘ä¼šè®¤çœŸå€¾å¬")

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# ========== 6. ç”¨æˆ·è¾“å…¥å¤„ç† ==========
if prompt := st.chat_input("è¯·åˆ†äº«ä½ çš„æ„Ÿå—..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
    
    # æ„é€ å¯¹è¯
    messages_for_model = [
        {"role": "system", "content": SYSTEM_PROMPT}
    ] + [
        {"role": m["role"], "content": m["content"]}
        for m in st.session_state.messages
    ]
    
    text = tokenizer.apply_chat_template(
        messages_for_model,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    # ========== 7. ç”Ÿæˆå›å¤ ==========
    with st.chat_message("assistant"):
        with st.spinner("ğŸ’­ æ€è€ƒä¸­..."):
            outputs = model.generate(
                **inputs,
                max_new_tokens=128,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id
            )
        
        # è§£ç 
        generated_tokens = outputs[0][inputs.input_ids.shape[1]:]
        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        # ========== 8. ç»ˆæè¿‡æ»¤ï¼ˆå››é‡ä¿é™©ï¼‰==========
        # ç¬¬1é‡ï¼šç§»é™¤ <think> æ ‡ç­¾ï¼ˆæ ¸å¿ƒé—®é¢˜ï¼ï¼‰
        response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)
        response = re.sub(r'<think>', '', response)  # ç§»é™¤æœªé—­åˆæ ‡ç­¾
        response = re.sub(r'</think>', '', response)  # ç§»é™¤é—­åˆæ ‡ç­¾
        
        # ç¬¬2é‡ï¼šç§»é™¤é›¶å®½å­—ç¬¦ï¼ˆå…¼å®¹å…¶ä»–æƒ…å†µï¼‰
        response = re.sub(r'[\u200B-\u200D\uFEFF\u2060\u202A-\u202E\u0000-\u001F]', '', response)
        
        # ç¬¬3é‡ï¼šç§»é™¤æ‹¬å·å†…å®¹
        response = re.sub(r'ï¼ˆ[^ï¼‰]*ï¼‰', '', response)
        response = re.sub(r'\([^)]*\)', '', response)
        
        # ç¬¬4é‡ï¼šç§»é™¤æ€è€ƒå¼€å¤´
        response = re.sub(r'^(å—¯|å¥½|å¥½çš„|æ˜ç™½äº†|æ€è€ƒï¼š|æ¨ç†ï¼š)[ï¼Œ,ï¼š:\s]*', '', response)
        
        # æ¸…ç†å¤šä½™æ¢è¡Œå’Œç©ºæ ¼
        response = re.sub(r'\n\s*\n', '\n', response)
        response = re.sub(r'^\s+|\s+$', '', response)
        
        # ç©ºå›å¤ä¿æŠ¤
        if not response or len(response) < 2:
            response = "æˆ‘åœ¨è¿™é‡Œå€¾å¬ä½ ï¼Œå¯ä»¥å¤šè¯´è¯´ä½ çš„æ„Ÿå—å—ï¼Ÿ"
        
        # ç›´æ¥æ˜¾ç¤ºçº¯å‡€ç»“æœ
        st.write(response)
        st.session_state.messages.append({"role": "assistant", "content": response})